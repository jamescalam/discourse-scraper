{"docs": "pytorch", "category": "autograd", "thread": "About the autograd category", "href": "https://discuss.pytorch.org/t/about-the-autograd-category/2932", "content": ["A category of posts relating to the autograd engine itself."]}
{"docs": "pytorch", "category": "autograd", "thread": "Custom loss function only returning grad of tensor(1.)", "href": "https://discuss.pytorch.org/t/custom-loss-function-only-returning-grad-of-tensor-1/142978", "content": ["Problem is as the title says. I\u2019m unsure of why this is happening, as when I\u2019m printing from the backward() method of my loss function, it prints the correct tensor for the grad, but when printing the grad from the training loop, it prints tensor(1.).\nIn the following code, I use a lot of code from a library called Qiskit. The main value that needs to be passed to the grad function is in state_fidelity(), which returns a scalar.\nExample of some output:\nTraining loss for batch 0: 0.008105693179143336\ntensor(1.)\n[[tensor(8.5487e-15), tensor(-2.8533e-14)]]\nTraining loss for batch 1: 0.568790329178134\ntensor(1.)\n[[tensor(0.), tensor(0.)]]\n\ntraining_loop:\nepochs = 10\n    \n    losses = []\n    \n    loss_fn = SingleQubitGaussianLoss.apply\n    \n    print(model)\n    \n    model.train(True)\n    \n    for epoch in range(epochs):\n        print(f\"\\nEPOCH {epoch}\")\n        for i, data in enumerate(data_loader):\n            inputs, labels = data\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_fn(outputs, inputs, labels)\n            loss.register_hook(lambda grad: print(grad))\n            if len(losses) < points:\n                losses.append(loss.item())\n                print(f\"Training loss for batch {i}: {loss.item()}\")\n            else:\n                comp = (loss.item() - losses[i]) / losses[i] * 100\n                losses[i] = loss.item()\n                print(f\"Training loss for batch {i}: {loss.item()}, {comp}% change\")\n            loss.backward()\n            optimizer.step()\n\nloss function:\nclass SingleQubitGaussianLoss(torch.autograd.Function):\n    # TODO somehow make duration a parameter\n    @staticmethod\n    def forward(ctx, params, inits, labels):\n        ctx.save_for_backward(params)\n        eval_inits = []\n        eval_labels = []\n        infidelity_list = []\n        for pred, init, label in zip(params, inits, labels):\n            amp, ln_sigma = pred\n            amp = amp.item()\n            ln_sigma = ln_sigma.item()\n            init_state = [torch.cos(init[0] / 2).item(), torch.exp(init[1] * 1.j).item() * \\\n                          torch.sin(init[0] / 2).item()]\n            eval_inits.append(init_state)\n            job = run_gaussian(duration=128,\n                               amp=process_amp(amp),\n                               sigma=process_ln_width(ln_sigma),\n                               init_state=init_state)\n            sv = job.result().get_statevector()\n            actual_sv = [label[0].item(), label[1].item(), 0]\n            eval_labels.append(actual_sv)\n            infidelity_list.append(1 - state_fidelity(sv, actual_sv))\n        ctx.eval_inits = eval_inits\n        ctx.eval_labels = eval_labels\n        ctx.infidelity_list = infidelity_list\n        return torch.DoubleTensor([sum(infidelity_list) / len(infidelity_list)])[0]\n            \n    \n    @staticmethod\n    def backward(ctx, h=1e-3):\n        params, = ctx.saved_tensors\n        jacobian = []\n        for pred, init_state, actual_sv, infidelity in \\\n            zip(params, ctx.eval_inits, ctx.eval_labels, ctx.infidelity_list):\n            amp, ln_sigma = pred\n            amp = amp.item()\n            ln_sigma = ln_sigma.item()\n            amp_job = run_gaussian(duration=128,\n                                   amp=process_amp(amp + h),\n                                   sigma=process_ln_width(ln_sigma),\n                                   init_state=init_state)\n            amp_sv = amp_job.result().get_statevector()\n            amp_infid = 1 - state_fidelity(actual_sv, amp_sv)\n            grad_amp = (amp_infid - infidelity) / h\n            \n            sigma_job = run_gaussian(duration=128,\n                                     amp=process_amp(amp),\n                                     sigma=process_ln_width(ln_sigma + h),\n                                     init_state=init_state)\n            sigma_sv = sigma_job.result().get_statevector()\n            sigma_infid = 1 - state_fidelity(actual_sv, sigma_sv)\n            grad_sigma = (sigma_infid - infidelity) / h\n            jacobian.append([grad_amp, grad_sigma])\n        print(jacobian)\n        return torch.DoubleTensor(jacobian), None, None\n\n\ndef process_amp(real: float, imag: float = 0):\n    # TODO implement imaginary amps\n    return 1 / (1 + math.exp(-1*real))\n\n\ndef process_ln_width(ln_width: float):\n    return ln_width\n\n\nmodel:\nclass SingleQubitPulseModel(torch.nn.Module):\n    def __init__(self):\n        super(SingleQubitPulseModel, self).__init__()\n        \n        # TODO Find better activation function, ReLU keeps producing 0 because amp is small\n        self.linear1 = torch.nn.Linear(2, 64)\n        self.relu1 = torch.nn.ReLU()\n        self.linear2 = torch.nn.Linear(64, 64)\n        self.relu2 = torch.nn.ReLU()\n        self.linear3 = torch.nn.Linear(64, 2)\n        self.relu3 = torch.nn.ReLU()\n    \n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu1(x)\n        x = self.linear2(x)\n        x = self.relu2(x)\n        x = self.linear3(x)\n        x = self.relu3(x)\n        return x", "If I understand the issue correctly, you are concerned why:\nloss.register_hook(lambda grad: print(grad))\nloss.backward()\n\ngives tensor(1.)?\nIf no gradient is passed to the backward function, it\u2019ll be automatically set as torch.ones(1) for you, as the initial gradient is defined as dLoss/dLoss = 1.", "(post deleted by author)", "Huh, oops my bad. How would I print dLoss/dy_1? where y_1 is one of the parameters from the output of the NN. The model won\u2019t train so I would like to see if the grads are being calculated correctly", "After calling loss.backward() you could check all gradients by accessing the .grad attribute of all parameters via:\nloss.backward()\nfor name, param in model.named_parameters():\n    print(name, param.grad)\n\nAlternatively, you could also calculate specific gradients via e.g. torch.autograd.grad.", "Ok that printed out something! It ended up printing out:\ntensor([ 1.2046e-02, -6.8818e+09], device='cuda:0', requires_grad=True)\nTraining loss for batch 9: 0.0186079985071379\n\nWhich looks about correct enough; however, my model doesn\u2019t seem to be training at all. I\u2019m comparing the values for each (unshuffled) batch from the last epoch, and I\u2019m getting something like:\nEPOCH 1\nTraining loss for batch 0: 0.27021969694780146, 0.0% change\nTraining loss for batch 1: 0.020840750138244823, 0.0% change\nTraining loss for batch 2: 0.8588083189094075, 0.0% change\nTraining loss for batch 3: 0.4859533358527053, 0.0% change\n\nI\u2019ve tried different optimizers as well as a learning rate of 10^14 in case my gradients were too small but it doesn\u2019t seem to change anything. Any ideas?"]}
{"docs": "pytorch", "category": "autograd", "thread": "How to get predict interval using Monte Carlo Dropout", "href": "https://discuss.pytorch.org/t/how-to-get-predict-interval-using-monte-carlo-dropout/143021", "content": ["Hi. I am trying to get standard deviation of predict value to describe predict interval using Monte Carlo Dropout. I can run my code without any problems. However I got standard deviation which all values are zero. I think there  are problems in dataset or algorithm. So give me some advices to fix my code.\nMy code is here.\nclass MyNet(nn.Module):\n    def __init__(self, x1, x2, p, m, mode=0):\n        super().__init__()\n        input_dim = p*m \n        hidden_dim = 128\n        self.dropout = nn.Dropout(p=0.1)\n        if mode == 1:\n            self.embed = Encoder1(x1, x2, p)\n        elif mode == 2:\n            self.embed = Encoder2(x1, x2, p)\n        else:\n            self.embed = Encoder(x1, x2, p)\n\n        self.share_block = nn.Sequential(\n            # nn.BatchNorm1d(input_dim),\n            nn.utils.weight_norm(nn.Linear(input_dim, hidden_dim)),\n            # nn.Dropout(p=0.1),\n            nn.SELU(),\n            # nn.BatchNorm1d(hidden_dim),\n            nn.utils.weight_norm(nn.Linear(hidden_dim, hidden_dim)),\n            # nn.Dropout(p=0.1),\n            nn.SELU(),\n            # nn.BatchNorm1d(hidden_dim),\n            nn.utils.weight_norm(nn.Linear(hidden_dim, hidden_dim)),\n            # nn.Dropout(p=0.1),\n            nn.SELU(),\n            # nn.BatchNorm1d(hidden_dim),\n            nn.utils.weight_norm(nn.Linear(hidden_dim, hidden_dim)),\n            # nn.Dropout(p=0.1),\n            nn.SELU()\n        )\n        self.head = nn.Sequential(\n            # nn.BatchNorm1d(hidden_dim),\n            nn.utils.weight_norm(nn.Linear(hidden_dim, 1))\n        )\n\n    def forward(self, x1, x2):\n        x1 = x1.to(device)\n        x2 = x2.to(device)\n        x = self.embed(x1, x2)\n        x = self.share_block(x)\n        _out = self.head(x)\n        _out = self.dropout(_out)\n       \n\n        return _out\n\n    def predict(self, x1, x2, alpha, samples=100):\n        x1 = x1.to(device)\n        x2 = x2.to(device)\n        # z_c = norm.ppf(alpha)\n        samples_ = []\n        for i in range(samples):\n            pred = self.forward(x1, x2).detach()\n            samples_ += pred\n            \n        # print(len(samples_))\n        pred_sample = torch.cat(samples_, dim=-1)\n        pred_sample = torch.reshape(pred_sample, (100, -1))\n        # print(pred_sample.shape)\n        pred_mean = torch.mean(pred_sample, dim=0)\n        print(pred_mean)\n        pred_std = np.sqrt(torch.var(pred_sample, dim=0))\n\n        return pred_mean, pred_std\n\ndef train_and_test_gpu(model, num_epochs, dataset):\n    # model.cuda()\n    print(model)\n    train_dataset = utils.data.Subset(dataset, train_idx)\n    train_loader = utils.data.DataLoader(\n        train_dataset, batch_size=64, worker_init_fn=seed_worker, generator=g, shuffle=True)\n\n    test_dataset = utils.data.Subset(dataset, test_idx)\n    test_loader = utils.data.DataLoader(\n        test_dataset, batch_size=64, shuffle=False)\n\n    optimizer = optim.Adam(model.parameters(), lr=1e-6)\n    criterion = nn.MSELoss()\n\n    # \u3053\u306escheduler\u306f\u304b\u306a\u308a\u7279\u6b8a\u3067scheduler.step()\u3092\u547c\u3073\u51fa\u3059\u4f4d\u7f6e\u3082\u9055\u3046\u306e\u3067\u6ce8\u610f\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=1e-3,\n        steps_per_epoch=len(train_loader),\n        epochs=num_epochs,\n    )\n\n    model.train()\n    for epoch in tqdm(range(num_epochs)):\n        for x1, x2, y in train_loader:\n            # print(x1.shape, x2.shape)\n\n            x1 = x1.to(device)\n            x2 = x2.to(device)\n            y = torch.Tensor(y).to(device)\n\n            # print(x1.is_cuda)\n            # print(x2.is_cuda)\n            # print(y.is_cuda)\n            optimizer.zero_grad()\n            y_pred = model(x1, x2)\n\n            # y_pred = torch.take_along_dim(y_pred, task)\n            loss = criterion(y_pred, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n    model.eval()\n    with torch.no_grad():\n        y_true_list = []\n        y_pred_list = []\n        pred_std_list = []\n        for x1, x2, y_true in test_loader:\n            x1 = torch.Tensor(x1).to(device)\n            x2 = torch.Tensor(x2).to(device)\n\n            y_pred, pred_std = model.predict(x1, x2, alpha=0.95)\n            # print(pred_std)\n            # y_pred = torch.take_along_dim(y_pred, task)\n            y_pred_list.extend(y_pred.detach().flatten().cpu().tolist())\n            y_true_list.extend(y_true.detach().cpu().tolist())\n            pred_std_list.extend(pred_std.detach().cpu().tolist())\n        # print(y_pred_list)\n    print(np.array(y_true_list).shape, np.array(y_pred_list).shape)\n    print(np.array(pred_std_list).shape)\n    plot_prediction(y_true_list, y_pred_list, color=\"red\")\n    return y_true_list, y_pred_list, pred_std_list\n\nand output standard deviation is here.\n[0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n\u30fb\u30fb\u30fb\n 0.0]", "@Takumi3 The monte carlo sounds interesting. Can you share a link of the paper/blog etc\nIn your code, I can see an issue here\nIn the predict function\nsamples_ = []\nfor i in range(samples):\n    pred = self.forward(x1, x2).detach()\n    samples_ += pred\n\nAlthough you are iterating over a range of 100,  you are performing the forward on x1,x2 without updating the weights. This will give you the same results for all the 100 iterations. You need to perform some weight updation like .step() etc"]}
{"docs": "pytorch", "category": "autograd", "thread": "Reuse part of the computation graph without loss.backward(retain_graph=True)", "href": "https://discuss.pytorch.org/t/reuse-part-of-the-computation-graph-without-loss-backward-retain-graph-true/143018", "content": ["(topic deleted by author)"]}
